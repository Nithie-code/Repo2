import pandas as pd
import matplotlib.pyplot as plt
import io
from PIL import Image, ImageDraw
import numpy as np
import os
from PIL import Image
import torch
from torch.utils.data import Dataset
import pandas as pd
from torch.utils.data import random_split
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

import torchvision
from torchvision import utils

path2csv = "D:/Projects/histopathologic-cancer-detection/train_labels.csv"
labels_df = pd.read_csv(path2csv)

print(labels_df.head())
print(labels_df['label'].value_counts())
# Histogram of Labels
labels_df['label'].hist()
plt.show()
malignantIds = labels_df.loc[labels_df['label'] == 1]['id'].values
# path to data
path2train = "D:/Projects/histopathologic-cancer-detection/train"
color = False
# Figure Size Settings
plt.rcParams['figure.figsize'] = (10.0, 10.0)
plt.subplots_adjust(wspace=0, hspace=0)
nrows, ncols = 3, 3
for i, id_ in enumerate(malignantIds[:nrows * ncols]):
    full_filenames = os.path.join(path2train, id_ + '.tif')
    # load image
    img = Image.open(full_filenames)
    # draw a 32*32 rectangle
    draw = ImageDraw.Draw(img)
    draw.rectangle(((32, 32), (64, 64)), outline="green")
    plt.subplot(nrows, ncols, i + 1)
    if color is True:
        plt.imshow(np.array(img))
    else:
        plt.imshow(np.array(img)[:, :, 0], cmap="gray")
    plt.axis('off')
plt.show()
print("image shape:", np.array(img).shape)
print("pixel values range from %s to %s" % (np.min(img), np.max(img)))

# fix torch random seed
torch.manual_seed(0)


class histoCancerDataset(Dataset):
    def __init__(self, data_dir, transform, data_type="train"):
        # path to images
        path2data = os.path.join(data_dir, data_type)
        # get a list of images
        filenames = os.listdir(path2data)
        # get the full path to images
        self.full_filenames = [os.path.join(path2data, f) for f in filenames]
        # labels are in a csv file named train_labels.csv
        csv_filename = data_type + "_labels.csv"
        path2csvLabels = os.path.join(data_dir, csv_filename)
        labels_df = pd.read_csv(path2csvLabels)
        # set data frame index to id
        labels_df.set_index("id", inplace=True)
        # obtain labels from data frame
        self.labels = [labels_df.loc[filename[:-4]].values[0] for
                       filename in filenames]
        self.transform = transform

    # return Length of dataset
    def __len__(self):
        # return size of dataset
        return len(self.full_filenames)

    # return image at specified index
    def __getitem__(self, idx):
        # open image, apply transforms and return with label
        image = Image.open(self.full_filenames[idx])  # PIL image
        image = self.transform(image)
        return image, self.labels[idx]


import torchvision.transforms as transforms

# Transform Function
data_transformer = transforms.Compose([transforms.ToTensor()])
data_dir = "D:/Projects/histopathologic-cancer-detection"
histo_dataset = histoCancerDataset(data_dir, data_transformer, "train")
print("Length of Dataset : ", len(histo_dataset))
img, label = histo_dataset[9]
print(img.shape, torch.min(img), torch.max(img))

# Splitting the dataset

from torch.utils.data import random_split

len_histo = len(histo_dataset)
len_train = int(0.8 * len_histo)
len_val = len_histo - len_train
train_ds, val_ds = random_split(histo_dataset, [len_train, len_val])
print("train dataset length:", len(train_ds))
print("validation dataset length:", len(val_ds))


# Helper function to display image
def show(img, y, color=False):
    npimg = img.numpy()
    # convert to H*W*C
    npimg_tr = np.transpose(npimg, (1, 2, 0))
    if color == False:
        npimg_tr = npimg_tr[:, :, 0]
        plt.imshow(npimg_tr, interpolation='nearest', cmap="gray")
    else:
        plt.imshow(npimg_tr, interpolation='nearest')
        plt.title("Label: " + str(y))


# Create grid  of Samples
grid_size = 4
rnd_inds = np.random.randint(0, len(train_ds), grid_size)
print("Image Indices : ", rnd_inds)
x_grid_train = [train_ds[i][0] for i in rnd_inds]
y_grid_train = [train_ds[i][1] for i in rnd_inds]
x_grid_train = utils.make_grid(x_grid_train, nrow=4, padding=2)
print(x_grid_train.shape)

plt.rcParams['figure.figsize'] = (10.0, 5)
show(x_grid_train, y_grid_train)
plt.show()

# Training transformation
train_transformer = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomRotation(45),
    transforms.RandomResizedCrop(96, scale=(0.8, 1.0), ratio=(1.0, 1.0)),
    transforms.ToTensor()])

# Val into tensors
val_transformer = transforms.Compose([transforms.ToTensor()])
# overwrite the transform functions
train_ds.transform = train_transformer
val_ds.transform = val_transformer
# DataLoaders
train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)
val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)


def accuracy(labels, out):
    return np.sum(out == labels) / float(len(labels))


# accuracy all zero predictions
# get labels for validation dataset
y_val = [y for _, y in val_ds]
acc_all_zeros = accuracy(y_val, np.zeros_like(y_val))
print("Accuracy all zero prediction: %.2f" % acc_all_zeros)


def findConv2dOutShape(H_in, W_in, conv, pool=2):
    # get conv arguments
    kernel_size = conv.kernel_size
    stride = conv.stride
    padding = conv.padding
    dilation = conv.dilation
    H_out = np.floor((H_in + 2 * padding[0] -
                      dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)
    W_out = np.floor((W_in + 2 * padding[1] -
                      dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)
    if pool:
        H_out /= pool
    W_out /= pool
    return int(H_out), int(W_out)


# Def Net Class
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self, params):
        super(Net, self).__init__()
        C_in, H_in, W_in = params["input_shape"]
        init_f = params["initial_filters"]
        num_fc1 = params["num_fc1"]
        num_classes = params["num_classes"]
        self.dropout_rate = params["dropout_rate"]
        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3)
        h, w = findConv2dOutShape(H_in, W_in, self.conv1)
        self.conv2 = nn.Conv2d(init_f, 2 * init_f, kernel_size=3)
        h, w = findConv2dOutShape(h, w, self.conv2)
        self.conv3 = nn.Conv2d(2 * init_f, 4 * init_f, kernel_size=3)
        h, w = findConv2dOutShape(h, w, self.conv3)
        self.conv4 = nn.Conv2d(4 * init_f, 8 * init_f, kernel_size=3)
        h, w = findConv2dOutShape(h, w, self.conv4)
        # compute the flatten size
        self.num_flatten = h * w * 8 * init_f
        self.fc1 = nn.Linear(self.num_flatten, num_fc1)
        self.fc2 = nn.Linear(num_fc1, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv3(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv4(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, self.num_flatten)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, self.dropout_rate, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


params_model = {
    "input_shape": (3, 96, 96),
    "initial_filters": 8,
    "num_fc1": 100,
    "dropout_rate": 0.25,
    "num_classes": 2,
}
cnn_model = Net(params_model)
# move model to cuda/gpu device

if torch.cuda.is_available():
    device = torch.device("cuda")
    cnn_model = cnn_model.to(device)

# Loss Function
loss_func = nn.NLLLoss(reduction="sum")
# fix random seed
torch.manual_seed(0)
n, c = 8, 2
y = torch.randn(n, c, requires_grad=True)
ls_F = nn.LogSoftmax(dim=1)
y_out = ls_F(y)
print(y_out.shape)
target = torch.randint(c, size=(n,))
print(target.shape)
loss = loss_func(y_out, target)
print(loss.item())

# Optimization
from torch import optim

opt = optim.Adam(cnn_model.parameters(), lr=3e-4)


def get_lr(opt):
    for param_group in opt.param_groups:
        return param_group['lr']


from torch.optim.lr_scheduler import ReduceLROnPlateau

lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.5,
                                 patience=20, verbose=1)

for i in range(100):
    lr_scheduler.step(1)


def metrics_batch(output, target):
    # get output class
    pred = output.argmax(dim=1, keepdim=True)
    # compare output class with target class
    corrects = pred.eq(target.view_as(pred)).sum().item()
    return corrects


def loss_batch(loss_func, output, target, opt=None):
    loss = loss_func(output, target)
    with torch.no_grad():
        metric_b = metrics_batch(output, target)
    if opt is not None:
        opt.zero_grad()
        loss.backward()
        opt.step()
    return loss.item(), metric_b


def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):
    running_loss = 0.0
    running_metric = 0.0
    len_data = len(dataset_dl.dataset)

    for xb, yb in dataset_dl:
        # move batch to device
        xb = xb.to(device)
        yb = yb.to(device)
        # get model output
        output = model(xb)
        # get loss per batch
        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)
        # update running loss
        running_loss += loss_b
        # update running metric
        if metric_b is not None:
            running_metric += metric_b
        # break the loop in case of sanity check
        if sanity_check is True:
            break
        # average loss value
    loss = running_loss / float(len_data)
    # average metric value
    metric = running_metric / float(len_data)
    return loss, metric


def train_val(model, params):
    # extract model parameters
    num_epochs = params["num_epochs"]
    loss_func = params["loss_func"]
    opt = params["optimizer"]
    train_dl = params["train_dl"]
    val_dl = params["val_dl"]
    sanity_check = params["sanity_check"]
    lr_scheduler = params["lr_scheduler"]
    path2weights = params["path2weights"]

    # history of loss values in each epoch
    loss_history = {"train": [], "val": [], }
    # history of metric values in each epoch
    metric_history = {"train": [], "val": [],
                      }
    # a deep copy of weights for the best performing model
    import copy
    # a deep copy of weights for the best performing model
    best_model_wts = copy.deepcopy(model.state_dict)
    best_loss = float('inf')

    # main Loop
    for epoch in range(num_epochs):
        current_lr = get_lr(opt)
        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs
                                                  - 1, current_lr))
        # Train
        model.train()
        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)
        # Collect and metric for training dataset
        loss_history["train"].append(train_loss)
        metric_history["train"].append(train_metric)
        # Model Evaluation
        model.eval()
        with torch.no_grad():
            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)
            # collect loss and metric for validation dataset
        loss_history["val"].append(val_loss)
        metric_history["val"].append(val_metric)
        # Store best weigths
        if val_loss < best_loss:
            best_loss = val_loss
            best_model_wts = copy.deepcopy(model.state_dict())
            # store weights into a local file
            torch.save(model.state_dict(), path2weights)
            print("Copied best model weights!")
            # Learning Rate
        lr_scheduler.step(val_loss)
        if current_lr != get_lr(opt):
            print("Loading best model weights!")
            model.load_state_dict(best_model_wts)
        print("train loss: %.6f, dev loss: %.6f, accuracy: %.2f"
              % (train_loss, val_loss, 100 * val_metric))
        print("-" * 10)
        # Load model best weights
    model.load_state_dict(best_model_wts)
    return model, loss_history, metric_history


loss_func = nn.NLLLoss(reduction="sum")
opt = optim.Adam(cnn_model.parameters(), lr=3e-4)
lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.5,
                                 patience=20, verbose=1)
# Model params

params_train = {
    "num_epochs": 100,
    "optimizer": opt,
    "loss_func": loss_func,
    "train_dl": train_dl,
    "val_dl": val_dl,
    "sanity_check": True,
    "lr_scheduler": lr_scheduler,
    "path2weights": "weights.pt",
}

# train and validate the model
cnn_model, loss_hist, metric_hist = train_val(cnn_model, params_train)

# Train_validation Progress
# Train-Validation Progress
num_epochs = params_train["num_epochs"]
# plot loss progress
plt.title("Train-Val Loss")
plt.plot(range(1, num_epochs + 1), loss_hist["train"], label="train")
plt.plot(range(1, num_epochs + 1), loss_hist["val"], label="val")
plt.ylabel("Loss")
plt.xlabel("Training Epochs")
plt.legend()
plt.show()
# plot accuracy progress
plt.title("Train-Val Accuracy")
plt.plot(range(1, num_epochs + 1), metric_hist["train"], label="train")
plt.plot(range(1, num_epochs + 1), metric_hist["val"], label="val")
plt.ylabel("Accuracy")
plt.xlabel("Training Epochs")
plt.legend()
plt.grid()
plt.show()

# MODEL DEPLOYMENT

# model parameters

import time

params_model = {
    "input_shape": (3, 96, 96),
    "initial_filters": 8,
    "num_fc1": 100,
    "dropout_rate": 0.25,
    "num_classes": 2,
}
# initialize model
cnn_model = Net(params_model)

# Load state_dict
# load state_dict into model
path2weights = "weights.pt"
cnn_model.load_state_dict(torch.load(path2weights))
cnn_model.eval()

# Cuda  Settings
if torch.cuda.is_available():
    device = torch.device("cuda")
    cnn_model = cnn_model.to(device)


# Deploy_model function
def deploy_model(model, dataset, device, num_classes=2, sanity_check=False):
    len_data = len(dataset)
    # initialize output tensor on CPU: due to GPU memory limits
    y_out = torch.zeros(len_data, num_classes)
    # initialize ground truth on CPU: due to GPU memory limits
    y_gt = np.zeros((len_data), dtype="uint8")
    # move model to device
    model = model.to(device)
    elapsed_times = []
    with torch.no_grad():
        for i in range(len_data):
            x, y = dataset[i]
            y_gt = dataset[i]
            start = time.time()
            y_out[i] = model(x.unsqueeze(0).to(device))
            elapsed = time.time() - start
            elapsed_times.append(elapsed)
            if sanity_check is True:
                break
    inference_time = np.mean(elapsed_times) * 100
    print("average inference time per image on %s: %.2f ms "
          % (device, inference_time))
    return y_out.numpy(), y_gt


# Deploy  Model

y_out, y_gt = deploy_model(cnn_model, val_ds, device=device, sanity_check=False)
print(y_out.shape, y_gt)
# Accuarcy
from sklearn.metrics import accuracy_score

# predictions
y_pred = np.argmax(y_out, axis=1)
print(y_pred.shape, y_gt.shape)
# Accuracy Computation
acc = accuracy_score(y_pred, y_out)
print("Accuracy : %.2f" % acc)
# Inference
path2csv = "D:/Projects/histopathologic-cancer-detection/test_labels.csv"
labels_df = pd.read_csv(path2csv)
print(labels_df.head())
# Create dataset Object   for test dataset
histo_test = histoCancerDataset(data_dir, val_transformer, data_type="test")
# Deploy Model on test dataset
y_test_out, _ = deploy_model(cnn_model, histo_test, device, sanity_check=False)
y_test_pred = np.argmax(y_test_out, axis=1)
print(y_test_pred.shape)
# Dipslay Few Images

grid_size = 4
rnd_inds = np.random.randint(0, len(histo_test), grid_size)
print("image indices:", rnd_inds)
x_grid_test = [histo_test[i][0] for i in range(grid_size)]
y_grid_test = [y_test_pred[i] for i in range(grid_size)]
x_grid_test = utils.make_grid(x_grid_test, nrow=4, padding=2)
print(x_grid_test.shape)
plt.title("Test Image ")
plt.rcParams['figure.figsize'] = (10.0, 5)
show(x_grid_test, y_grid_test)
plt.show()

